{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RFbfM4unTX0G"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "import io\n",
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import random as python_random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn.metrics as sklm\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "seed=19\n",
        "np.random.seed(seed)\n",
        "python_random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks9PJZN8NtFk",
        "outputId": "464db485-20e6-4993-dc80-ba7fba193d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KXpx2sA5QWh3"
      },
      "outputs": [],
      "source": [
        "base_path=\"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6cHRC5MnlDq"
      },
      "source": [
        "### Start here if you have the CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "61xT-WeonrJx"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(base_path+\"Extracted_Embeddings/processed_mimic_df.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIuZyci2n36j",
        "outputId": "be8674b9-de28-42ef-9720-4bc40098035c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'path', 'subject_id', 'study_id', 'dicom_id', 'split',\n",
              "       'gender', 'insurance', 'anchor_age', 'age_decile', 'race',\n",
              "       'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema',\n",
              "       'Enlarged_Cardiomediastinum', 'Fracture', 'Lung_Lesion', 'Lung_Opacity',\n",
              "       'No_Finding', 'Pleural_Effusion', 'Pleural_Other', 'Pneumonia',\n",
              "       'Pneumothorax', 'Support_Devices'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EVZXqYhNTf4"
      },
      "source": [
        "### Race prediction using embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbRts6TcLLN-"
      },
      "source": [
        "### Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lx3I21Wjj2rf"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf  # Import TensorFlow\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "def train_model(dataframe, target_column):\n",
        "    \"\"\"\n",
        "    Trains a neural network model to predict a binary outcome based on embeddings using GPU.\n",
        "\n",
        "    :param dataframe: A pandas DataFrame containing an 'embeddings' column and a binary target column.\n",
        "    :param target_column: The name of the column in dataframe that is the target binary outcome to predict.\n",
        "    :return: A tuple of the trained model and its accuracy on the test set.\n",
        "    \"\"\"\n",
        "    # Isolate the embeddings and the target column\n",
        "    X = np.stack(dataframe['embeddings'].values)\n",
        "    y = dataframe[target_column].values\n",
        "\n",
        "    # Encode labels if they are not already numeric\n",
        "    if not np.issubdtype(y.dtype, np.number):\n",
        "        encoder = LabelEncoder()\n",
        "        y = encoder.fit_transform(y)\n",
        "\n",
        "    # Split the data into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Build the model on GPU\n",
        "    with tf.device('/GPU:0'):  # Specify GPU device\n",
        "        model = Sequential([\n",
        "            Dense(512, input_dim=X.shape[1], activation='relu'),\n",
        "            Dense(256, activation='relu'),\n",
        "            Dense(128, activation='relu'),\n",
        "            Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "        # Train the model\n",
        "        history = model.fit(X_train, y_train, epochs=10, batch_size=50, validation_split=0.2, verbose=1)\n",
        "\n",
        "        # Evaluate the model on the test data\n",
        "        loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    return model, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "k5MxMxSO6mwx"
      },
      "outputs": [],
      "source": [
        "## real one\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Read the CSV file\n",
        "# df = pd.read_csv(\"C:\\\\Users\\\\atauqeer\\\\Desktop\\\\Project\\\\processed_mimic_df.csv\")\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Extracted_Embeddings/processed_mimic_df.csv\")\n",
        "\n",
        "# Load the .npz file(Uncomment line for first run)\n",
        "npz_data = np.load(\"/content/drive/MyDrive/embeddings_and_paths_2.npz\", mmap_mode='r')\n",
        "# npz_data = np.load(\"C:\\\\Users\\\\atauqeer\\\\Desktop\\\\Project\\\\embeddings_and_paths_2.npz\", mmap_mode='r')\n",
        "\n",
        "file_paths = npz_data['file_paths']\n",
        "embeddings = npz_data['embeddings']\n",
        "\n",
        "# Reshape embeddings and select corresponding file paths\n",
        "all_embeddings = embeddings.reshape(-1, 1376)\n",
        "selected_file_paths = df['path'].to_list()\n",
        "file_names = [path.split('/')[-1] for path in selected_file_paths]\n",
        "file_paths =[i.split('\\\\')[-1] for i in file_paths]\n",
        "file_names_set = set(file_names)\n",
        "selected_indices = [idx for idx, path in enumerate(file_paths) if path in file_names_set]\n",
        "\n",
        "selected_embeddings = all_embeddings[selected_indices]\n",
        "\n",
        "# Create final DataFrame\n",
        "final_df = pd.DataFrame({\n",
        "    'file_path': [file_paths[idx] for idx in selected_indices],\n",
        "    'embeddings': [emb.tolist() for emb in selected_embeddings]\n",
        "})\n",
        "## for unbalanced\n",
        "df['path']  = [path.split('/')[-1] for path in df['path']]\n",
        "final_df = pd.merge(df, final_df, left_on='path', right_on='file_path', how='inner')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_1mZIuvWl3J4"
      },
      "outputs": [],
      "source": [
        "### Balancing Dataframe\n",
        "\n",
        "# Function to balance dataset by disease, race, and label\n",
        "\n",
        "def balance_dataset_by_disease_race_label(df, disease_labels, races=['BLACK/AFRICAN AMERICAN', 'WHITE']):\n",
        "    \"\"\"\n",
        "    Balances DataFrame 'df' by disease labels and races. Returns dictionary with balanced DataFrames for each disease.\n",
        "\n",
        "    Parameters:\n",
        "    df (DataFrame): Input DataFrame.\n",
        "    disease_labels (list): List of disease labels.\n",
        "    races (list): List of races to consider. Default is ['BLACK/AFRICAN AMERICAN', 'WHITE'].\n",
        "\n",
        "    Returns:\n",
        "    dict: Balanced DataFrames for each disease label.\n",
        "    \"\"\"\n",
        "    balanced_datasets = {}\n",
        "\n",
        "    for disease_label in disease_labels:\n",
        "        sampled_dfs = []\n",
        "        min_counts = []\n",
        "\n",
        "        for label in [0, 1]:\n",
        "            for race in races:\n",
        "                count = df[(df[disease_label] == label) & (df['race'] == race)]['subject_id'].nunique()\n",
        "                min_counts.append(count)\n",
        "\n",
        "        min_count = min(min_counts)\n",
        "\n",
        "        for label in [0, 1]:\n",
        "            for race in races:\n",
        "                cases_df = df[(df[disease_label] == label) & (df['race'] == race)]\n",
        "                sampled_patients = cases_df['subject_id'].drop_duplicates().sample(n=min_count, random_state=42)\n",
        "                sampled_cases_df = cases_df[cases_df['subject_id'].isin(sampled_patients)]\n",
        "                sampled_dfs.append(sampled_cases_df)\n",
        "\n",
        "        balanced_df = pd.concat(sampled_dfs, ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "        balanced_datasets[f'{disease_label}_balanced'] = balanced_df\n",
        "\n",
        "    return balanced_datasets\n",
        "\n",
        "balanced_datasets = balance_dataset_by_disease_race_label(final_df, final_df.columns[11:24])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df = balanced_datasets[list(balanced_datasets.keys())[8]]\n",
        "# final_balanced_df = pd.merge(balanced_df[[\"path\", ]], final_df, left_on='path', right_on='path', how='inner')"
      ],
      "metadata": {
        "id": "ZnKj-pmtoWhw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "us1Snz2hrZqI",
        "outputId": "6b17f4ac-7995-462a-bdf9-99260d15b821"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'path', 'subject_id', 'study_id', 'dicom_id', 'split',\n",
              "       'gender', 'insurance', 'anchor_age', 'age_decile', 'race',\n",
              "       'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema',\n",
              "       'Enlarged_Cardiomediastinum', 'Fracture', 'Lung_Lesion', 'Lung_Opacity',\n",
              "       'No_Finding', 'Pleural_Effusion', 'Pleural_Other', 'Pneumonia',\n",
              "       'Pneumothorax', 'Support_Devices', 'file_path', 'embeddings'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3e1aPD9SpApS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "def calculate_auc_fpr(dataframe, target_column, protected_group_column, trained_model):\n",
        "    \"\"\"\n",
        "    Calculate the AUC and FPR for each protected group in the dataframe.\n",
        "\n",
        "    :param dataframe: A pandas DataFrame containing an 'embeddings' column, a binary target column,\n",
        "                      and a column specifying the protected group.\n",
        "    :param target_column: The name of the column in the dataframe that is the target binary outcome to predict.\n",
        "    :param protected_group_column: The name of the column specifying the protected group.\n",
        "    :param trained_model: The trained model to use for predictions.\n",
        "    :return: A dictionary containing AUC and FPR for each protected group.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    protected_groups = dataframe[protected_group_column].unique()\n",
        "    for group in protected_groups:\n",
        "        # Filter dataframe for the specific protected group\n",
        "        group_df = dataframe[dataframe[protected_group_column] == group]\n",
        "        # Isolate embeddings and target column\n",
        "        X_group = np.stack(group_df['embeddings'].values)\n",
        "        y_group = group_df[target_column].values\n",
        "        # Predict probabilities\n",
        "        y_pred_proba = trained_model.predict(X_group)\n",
        "        # Calculate AUC\n",
        "        auc = roc_auc_score(y_group, y_pred_proba)\n",
        "        # Calculate FPR\n",
        "        fpr = roc_curve(y_group, y_pred_proba)\n",
        "        fpr = fpr[1]  # FPR for positive class\n",
        "        # Store results\n",
        "        results[group] = {'AUC': auc, 'FPR': fpr}\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMB3EPJephmR",
        "outputId": "b40738ab-2c51-4415-ce2e-d48645f2e361",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "507/507 [==============================] - 8s 7ms/step - loss: 0.4956 - accuracy: 0.7777 - val_loss: 0.4645 - val_accuracy: 0.7897\n",
            "Epoch 2/10\n",
            "507/507 [==============================] - 2s 5ms/step - loss: 0.4643 - accuracy: 0.7899 - val_loss: 0.4649 - val_accuracy: 0.7907\n",
            "Epoch 3/10\n",
            "507/507 [==============================] - 2s 5ms/step - loss: 0.4563 - accuracy: 0.7925 - val_loss: 0.4605 - val_accuracy: 0.7934\n",
            "Epoch 4/10\n",
            "507/507 [==============================] - 2s 5ms/step - loss: 0.4533 - accuracy: 0.7948 - val_loss: 0.4942 - val_accuracy: 0.7758\n",
            "Epoch 5/10\n",
            "507/507 [==============================] - 2s 5ms/step - loss: 0.4490 - accuracy: 0.7985 - val_loss: 0.4699 - val_accuracy: 0.7874\n",
            "Epoch 6/10\n",
            "507/507 [==============================] - 3s 7ms/step - loss: 0.4487 - accuracy: 0.7972 - val_loss: 0.4578 - val_accuracy: 0.7935\n",
            "Epoch 7/10\n",
            "507/507 [==============================] - 2s 5ms/step - loss: 0.4445 - accuracy: 0.8001 - val_loss: 0.4598 - val_accuracy: 0.7940\n",
            "Epoch 8/10\n",
            "507/507 [==============================] - 2s 5ms/step - loss: 0.4431 - accuracy: 0.8009 - val_loss: 0.4599 - val_accuracy: 0.7953\n",
            "Epoch 9/10\n",
            "507/507 [==============================] - 2s 5ms/step - loss: 0.4388 - accuracy: 0.8025 - val_loss: 0.4743 - val_accuracy: 0.7891\n",
            "Epoch 10/10\n",
            "507/507 [==============================] - 2s 5ms/step - loss: 0.4380 - accuracy: 0.8028 - val_loss: 0.4658 - val_accuracy: 0.7948\n",
            "616/616 [==============================] - 1s 2ms/step\n",
            "623/623 [==============================] - 1s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "def train_and_evaluate_models(balanced_df, unbalanced_df, dataset_index):\n",
        "    \"\"\"\n",
        "    Trains models on both balanced and unbalanced datasets for the specified dataset index,\n",
        "    calculates FPR disparity, and displays training accuracy and FPR disparity metrics.\n",
        "\n",
        "    Args:\n",
        "    - balanced_datasets: Dictionary of balanced DataFrames.\n",
        "    - unbalanced_df: DataFrame of the unbalanced dataset.\n",
        "    - dataset_index: Index of the dataset to process.\n",
        "    \"\"\"\n",
        "    keys = list(balanced_datasets.keys())\n",
        "    if dataset_index < len(keys):\n",
        "        k = keys[dataset_index]\n",
        "        # print(f\"Dataset: {k}\")\n",
        "\n",
        "        # # Process balanced dataset\n",
        "        # balanced_df = balanced_datasets[k]\n",
        "        trained_model_balanced, accuracy_balanced = train_model(balanced_df, target_column=\"No_Finding\")\n",
        "        auc_fpr_results_balanced = calculate_auc_fpr(balanced_df, target_column=\"No_Finding\", protected_group_column='race', trained_model=trained_model_balanced)\n",
        "\n",
        "        # Process unbalanced dataset\n",
        "        trained_model_unbalanced, accuracy_unbalanced = train_model(unbalanced_df, target_column=\"No_Finding\")\n",
        "        auc_fpr_results_unbalanced = calculate_auc_fpr(unbalanced_df, target_column=\"No_Finding\", protected_group_column='race', trained_model=trained_model_unbalanced)\n",
        "\n",
        "        # Display results for balanced dataset\n",
        "        print(\"Balanced Dataset:\")\n",
        "        print(f\"Training Accuracy: {accuracy_balanced:.4f}\")\n",
        "        print(\"FPR Disparity:\")\n",
        "        for group, metrics in auc_fpr_results_balanced.items():\n",
        "            print(f\"Protected Group: {group}, AUC: {metrics['AUC']:.4f}\")\n",
        "\n",
        "        # Display results for unbalanced dataset\n",
        "        print(\"\\nUnbalanced Dataset:\")\n",
        "        print(f\"Training Accuracy: {accuracy_unbalanced:.4f}\")\n",
        "        print(\"FPR Disparity:\")\n",
        "        for group, metrics in auc_fpr_results_unbalanced.items():\n",
        "            print(f\"Protected Group: {group}, AUC: {metrics['AUC']:.4f}\")\n",
        "        print()\n",
        "    else:\n",
        "        print(\"Invalid dataset index.\")\n",
        "\n",
        "\n",
        "train_and_evaluate_models(balanced_df, unbalanced_df=final_df, dataset_index=8)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df.columns"
      ],
      "metadata": {
        "id": "La2qudPcwi-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvX35R1o6mwy"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def calculate_fpr(df, target_column, protected_group_column, trained_model):\n",
        "    \"\"\"\n",
        "    Calculate False Positive Rate (FPR) for each protected group.\n",
        "    \"\"\"\n",
        "    embeddings = df['embeddings']  # Assuming 'embeddings' contains the necessary data\n",
        "    embeddings = np.array([[np.array(sublist) for sublist in sublist_list] for sublist_list in embeddings])\n",
        "\n",
        "    predictions = trained_model.predict(embeddings)\n",
        "    true_labels = df[target_column]\n",
        "    groups = df[protected_group_column].unique()\n",
        "    fpr_results = {}\n",
        "\n",
        "    for group in groups:\n",
        "        group_indices = df[df[protected_group_column] == group].index\n",
        "        group_predictions = predictions[group_indices]\n",
        "        group_true_labels = true_labels[group_indices]\n",
        "        tn, fp, fn, tp = confusion_matrix(group_true_labels, group_predictions).ravel()\n",
        "        fpr = fp / (fp + tn) if (fp + tn) != 0 else 0  # Avoid division by zero\n",
        "        fpr_results[group] = fpr\n",
        "\n",
        "    return fpr_results\n",
        "\n",
        "\n",
        "def evaluate_model_fpr(balanced_datasets, unbalanced_df, target_dataset_index):\n",
        "    \"\"\"\n",
        "    Evaluates FPR disparity and training accuracy for both balanced and unbalanced datasets\n",
        "    at the specified index. Only the unbalanced dataset processing is included as per the provided code.\n",
        "\n",
        "    Args:\n",
        "    - balanced_datasets: Dictionary containing balanced DataFrames keyed by disease labels.\n",
        "    - unbalanced_df: DataFrame containing the unbalanced dataset.\n",
        "    - target_dataset_index: Index of the dataset to evaluate (0-based).\n",
        "    \"\"\"\n",
        "    keys = list(balanced_datasets.keys())\n",
        "    if target_dataset_index < len(keys):\n",
        "        k = keys[target_dataset_index]\n",
        "        print(f\"Dataset: {k}\")\n",
        "\n",
        "        # Train model and calculate FPR on the unbalanced dataset\n",
        "        trained_model_unbalanced, accuracy_unbalanced = train_model(unbalanced_df, target_column=k)\n",
        "        fpr_results_unbalanced = calculate_fpr(unbalanced_df, target_column=k, protected_group_column='race', trained_model=trained_model_unbalanced)\n",
        "\n",
        "        # Display training accuracy and FPR disparity for the unbalanced dataset\n",
        "        print(\"Unbalanced Dataset:\")\n",
        "        print(f\"Training Accuracy: {accuracy_unbalanced:.4f}\")\n",
        "        print(\"FPR Disparity:\")\n",
        "        for group, fpr in fpr_results_unbalanced.items():\n",
        "            print(f\"Protected Group: {group}, FPR: {fpr:.4f}\")\n",
        "        print()\n",
        "    else:\n",
        "        print(\"Invalid dataset index.\")\n",
        "\n",
        "evaluate_model_fpr(balanced_datasets, unbalanced_df=final_df, target_dataset_index=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ujP4BzS6mwy"
      },
      "outputs": [],
      "source": [
        "# final_df.to_csv(\"embeddings.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSC6ZIEp6mwz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "TZc9oLednPeG"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}